{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import zipfile\n",
    "import sklearn\n",
    "import shutil\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import zipfile\n",
    "import tempfile\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_image(image_path):\n",
    "    \"\"\"\n",
    "    reads image from path and shape it into (c,h,w)\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path, 0)  # Assuming the image is grayscale\n",
    "    img_array = np.array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add channel dimension\n",
    "    return img_array\n",
    "\n",
    "\n",
    "def format_name(zip_file_name):\n",
    "    \"\"\"\n",
    "    reads zip file name and format it into lower case\n",
    "    \"\"\"\n",
    "    formatted_name = zip_file_name.replace(\".zip\", \"\").replace(\"_\", \" \").lower()\n",
    "    return formatted_name\n",
    "\n",
    "\n",
    "class BldgDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path=\"./data/experiments/casestudy.zip\",\n",
    "        mode=\"train\",\n",
    "        transform=None,\n",
    "        seq_len=5,\n",
    "        num_seq=6,\n",
    "        num_frame=30,\n",
    "        seed=42,\n",
    "    ):\n",
    "        super(BldgDataset, self).__init__()\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "        self.seq_len = seq_len\n",
    "        self.num_seq = num_seq\n",
    "        self.building_names = []\n",
    "        self.num_frame = num_frame  # 30 frames each path\n",
    "\n",
    "        # Load dataset\n",
    "        self.data = []\n",
    "        self.mean = 0.0\n",
    "        self.std = 0.0\n",
    "        self.load_dataset()\n",
    "        self.split_data(seed)\n",
    "\n",
    "    def load_dataset(self):\n",
    "        sum_pixels = np.float64(0)\n",
    "        sum_pixels_squared = np.float64(0)\n",
    "        pixel_count = np.float64(0)\n",
    "        # Extract the main zip file if needed\n",
    "        with zipfile.ZipFile(self.data_path, \"r\") as main_zip:\n",
    "            with tempfile.TemporaryDirectory() as temp_dir:\n",
    "                main_zip.extractall(temp_dir)\n",
    "\n",
    "                # Iterate over each building sequence label zip file inside the main directory\n",
    "                for bldg_zip_name in main_zip.namelist():\n",
    "                    bldg_route_label = format_name(bldg_zip_name)\n",
    "                    route = bldg_route_label[\n",
    "                        -1\n",
    "                    ]  # The last character is the route label a,b,c,d\n",
    "                    bldg = bldg_route_label[:-2]\n",
    "                    if bldg not in self.building_names:\n",
    "                        self.building_names.append(bldg)\n",
    "\n",
    "                    # Extract the building sequence label zip file\n",
    "                    bldg_zip_path = os.path.join(temp_dir, bldg_zip_name)\n",
    "                    with zipfile.ZipFile(bldg_zip_path, \"r\") as bldg_zip:\n",
    "                        bldg_temp_dir = os.path.join(temp_dir, bldg_route_label)\n",
    "                        bldg_zip.extractall(bldg_temp_dir)\n",
    "\n",
    "                        # Iterate over path folders\n",
    "                        for path_folder in tqdm(\n",
    "                            sorted(os.listdir(bldg_temp_dir)),\n",
    "                            desc=f\"Loading {bldg}\",\n",
    "                            unit=\"path\",\n",
    "                        ):\n",
    "                            if path_folder.startswith(\"path\"):\n",
    "                                path_images = []\n",
    "                                path_folder_full = os.path.join(\n",
    "                                    bldg_temp_dir, path_folder\n",
    "                                )\n",
    "\n",
    "                                for frame in range(\n",
    "                                    self.num_frame\n",
    "                                ):  # Assuming 30 frames per path\n",
    "                                    img_filename = f\"panoramic_{frame:02d}.png\"\n",
    "                                    img_path = os.path.join(\n",
    "                                        path_folder_full, img_filename\n",
    "                                    )\n",
    "                                    if os.path.exists(img_path):\n",
    "                                        img_array = read_image(img_path).astype(\n",
    "                                            np.float64\n",
    "                                        )\n",
    "                                        path_images.append(img_array)\n",
    "                                        # Update the sums for mean and std calculation\n",
    "                                        sum_pixels += img_array.sum()\n",
    "                                        sum_pixels_squared += (img_array**2).sum()\n",
    "                                        pixel_count += img_array.size\n",
    "\n",
    "                                # Only consider complete sequences with 30 frames\n",
    "                                if len(path_images) != self.num_frame:\n",
    "                                    print(\n",
    "                                        f\"Error: {bldg}, Route {route}, Path {path_folder} does not have 30 images.\"\n",
    "                                    )\n",
    "                                    return\n",
    "                                else:\n",
    "                                    self.data.append(\n",
    "                                        {\n",
    "                                            \"images\": np.concatenate(\n",
    "                                                path_images, axis=0\n",
    "                                            ),\n",
    "                                            \"path\": int(\n",
    "                                                re.search(r\"\\d+\", path_folder).group()\n",
    "                                            ),\n",
    "                                            \"route\": route,\n",
    "                                            \"bldg\": bldg,\n",
    "                                        }\n",
    "                                    )\n",
    "        self.mean = sum_pixels / pixel_count\n",
    "        variance = (sum_pixels_squared / pixel_count) - (self.mean**2)\n",
    "        if variance < 0:\n",
    "            if np.isclose(variance, 0):\n",
    "                self.std = 0\n",
    "            else:\n",
    "                raise ValueError(f\"Calculated negative variance: {variance}\")\n",
    "        else:\n",
    "            self.std = np.sqrt(variance)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        imgs = item[\"images\"]\n",
    "        t_imgs = imgs.reshape(-1, 1, 30, 60)  # num_frame, C, H, W\n",
    "        # Apply transform if provided\n",
    "        if self.transform:\n",
    "            t_imgs = self.transform(t_imgs)\n",
    "        t_imgs = np.stack(t_imgs, axis=0)\n",
    "        t_imgs = torch.from_numpy(t_imgs).float()\n",
    "        # normalize\n",
    "        t_imgs = (t_imgs - self.mean) / self.std\n",
    "        (C, H, W) = t_imgs[0].size()\n",
    "        t_imgs = t_imgs.view(self.num_seq, self.seq_len, C, H, W).transpose(\n",
    "            1, 2\n",
    "        )  # num_seq,C,seq_len,H,W\n",
    "\n",
    "        # Return data as a dictionary\n",
    "        return {\n",
    "            \"t_imgs\": t_imgs,\n",
    "            \"imgs\": imgs,\n",
    "            \"path\": item[\"path\"],\n",
    "            \"route\": item[\"route\"],\n",
    "            \"bldg\": item[\"bldg\"],\n",
    "        }\n",
    "\n",
    "    def split_data(self, seed):\n",
    "        # Split the data into training and validation sets\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(self.data)  # Shuffle the data\n",
    "\n",
    "        split_index = int(len(self.data) * 0.8)\n",
    "        if self.mode == \"train\":\n",
    "            self.data = self.data[:split_index]\n",
    "        elif self.mode == \"val\":\n",
    "            self.data = self.data[split_index:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def find_indices(self, bldg_name, route, path_number):\n",
    "        \"\"\"\n",
    "        Finds the indices of the data items that match the given building, route, and path number.\n",
    "\n",
    "        :param bldg_name: The name of the building (formatted as 'caracalla baths', for example).\n",
    "        :param route: The route label (a single character like 'a', 'b', etc.).\n",
    "        :param path_number: The path number (an integer).\n",
    "        :return: A list of indices that match the criteria.\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        for idx, item in enumerate(self.data):\n",
    "            if (\n",
    "                item[\"bldg\"].lower() == bldg_name.lower()\n",
    "                and item[\"route\"].lower() == route.lower()\n",
    "                and item[\"path\"] == path_number\n",
    "            ):\n",
    "                indices.append(idx)\n",
    "        return indices\n",
    "\n",
    "    def plot_one_sequence(self, idx):\n",
    "        \"\"\"take idx of the data , plot the sequence\"\"\"\n",
    "        path = self[idx][\"path\"]\n",
    "        imgs = self[idx][\"imgs\"]\n",
    "        bldg = self[idx][\"bldg\"]\n",
    "        route = self[idx][\"route\"]\n",
    "        for img in imgs:\n",
    "            fig = plt.figure()\n",
    "            ax = fig.add_subplot(1, 1, 1)\n",
    "            ax.set_title(f\"{bldg} route {route} path {path}\")\n",
    "            plt.imshow(img.reshape(30, 60), cmap=\"gray\", vmin=0, vmax=255)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 262.28path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 252.62path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 267.88path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 258.91path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 249.92path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 269.39path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 252.80path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 244.84path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 262.34path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 243.83path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 269.62path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 251.69path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 256.96path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 272.90path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 252.75path/s]\n",
      "Loading trenton bath house: 100%|██████████| 100/100 [00:00<00:00, 259.82path/s]\n",
      "Loading trenton bath house: 100%|██████████| 100/100 [00:00<00:00, 252.81path/s]\n",
      "Loading trenton bath house: 100%|██████████| 100/100 [00:00<00:00, 245.79path/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['caracalla baths', 'india institute of management', 'pantheon', 'trajans market', 'trenton bath house']\n",
      "99.77257469135803\n",
      "92.27697774383148\n"
     ]
    }
   ],
   "source": [
    "# Example of how to use the dataset\n",
    "dataset = BldgDataset()\n",
    "print(dataset.building_names)\n",
    "print(dataset.mean)\n",
    "print(dataset.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 30, 60)\n",
      "torch.Size([6, 1, 5, 30, 60])\n",
      "caracalla baths\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"imgs\"].shape)\n",
    "print(dataset[0][\"t_imgs\"].shape)\n",
    "print(dataset[0][\"bldg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.plot_one_sequence(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import numbers\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrightnessJitter(object):  # 0.5 to 5 is a good range\n",
    "    def __init__(self, brightness=0, consistent=True, p=0.5):\n",
    "        self.brightness = self._check_input(brightness, \"brightness\")\n",
    "        self.consistent = consistent\n",
    "        self.threshold = p\n",
    "\n",
    "    def _check_input(\n",
    "        self, value, name, center=1, bound=(0, float(\"inf\")), clip_first_on_zero=True\n",
    "    ):\n",
    "        if isinstance(value, numbers.Number):\n",
    "            if value < 0:\n",
    "                raise ValueError(\n",
    "                    \"If {} is a single number, it must be non negative.\".format(name)\n",
    "                )\n",
    "            value = [center - value, center + value]\n",
    "            if clip_first_on_zero:\n",
    "                value[0] = max(value[0], 0)\n",
    "        elif isinstance(value, (tuple, list)) and len(value) == 2:\n",
    "            if not bound[0] <= value[0] <= value[1] <= bound[1]:\n",
    "                raise ValueError(\"{} values should be between {}\".format(name, bound))\n",
    "        else:\n",
    "            raise TypeError(\n",
    "                \"{} should be a single number or a list/tuple with lenght 2.\".format(\n",
    "                    name\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if value[0] == value[1] == center:\n",
    "            value = None\n",
    "        return value\n",
    "\n",
    "    @staticmethod\n",
    "    def get_params(brightness):\n",
    "        transforms = []\n",
    "\n",
    "        if brightness is not None:\n",
    "            brightness_factor = random.uniform(brightness[0], brightness[1])\n",
    "            transforms.append(lambda img: img * brightness_factor)\n",
    "\n",
    "        transform = torchvision.transforms.Compose(transforms)\n",
    "\n",
    "        return transform\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        if random.random() < self.threshold:  # do BrightnessJitter\n",
    "            if self.consistent:\n",
    "                transform = self.get_params(self.brightness)\n",
    "                return [transform(i) for i in imgmap]\n",
    "            else:\n",
    "                result = []\n",
    "                for img in imgmap:\n",
    "                    transform = self.get_params(self.brightness)\n",
    "                    result.append(transform(img))\n",
    "                return result\n",
    "        else:  # don't do BrightnessJitter, do nothing\n",
    "            return imgmap\n",
    "\n",
    "    def __repr__(self):\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        format_string += \"brightness={0}\".format(self.brightness)\n",
    "        format_string += \")\"\n",
    "        return format_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to test\n",
    "class RandomHorizontalShift:\n",
    "    def __init__(self, max_shift=30, p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            max_shift (int): the maximum number of pixels for the horizontal shift.\n",
    "            p (float): probability of applying the shift. Default is 0.5.\n",
    "        \"\"\"\n",
    "        self.max_shift = max_shift\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        return [self.horizontal_shift(img) for img in imgmap]\n",
    "\n",
    "    def horizontal_shift(self, img):\n",
    "        \"\"\"\n",
    "        Shift the image horizontally by a random number of pixels and wrap around.\n",
    "        Args:\n",
    "            img (ndarray): the input image as a numpy array.\n",
    "        Returns:\n",
    "            img (ndarray): the transformed image as a numpy array.\n",
    "        \"\"\"\n",
    "        # Check if we should apply the shift based on the probability p\n",
    "        if random.random() < self.p:\n",
    "            shift = random.randint(0, self.max_shift)\n",
    "            shifted_np_img = np.roll(img, shift, axis=2)  # roll along width dimension\n",
    "            return shifted_np_img\n",
    "        return img  # return original image if not shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomHorizontalFlip:  # choose consistent to be false\n",
    "    def __init__(self, consistent=True, p=0.5):\n",
    "        self.consistent = consistent\n",
    "        self.threshold = p\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        if self.consistent:\n",
    "            if random.random() > self.threshold:\n",
    "                return [np.flip(i, axis=[0, 2]) for i in imgmap]\n",
    "            else:\n",
    "                return imgmap\n",
    "        else:\n",
    "            result = []\n",
    "            for i in imgmap:\n",
    "                if random.random() > self.threshold:\n",
    "                    result.append(np.flip(i, axis=[0, 2]))\n",
    "                else:\n",
    "                    result.append(i)\n",
    "            assert len(result) == len(imgmap)\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Of Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[0][\"imgs\"][:10]\n",
    "brightness_jitter_transform = BrightnessJitter(\n",
    "    brightness=[0.5, 5], consistent=False, p=1\n",
    ")\n",
    "brightness_jittered_dataset = brightness_jitter_transform(test_dataset)\n",
    "print(len(brightness_jittered_dataset[0]))\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    img = brightness_jittered_dataset[i]\n",
    "    ax.set_title(f\"img {i}\")\n",
    "    ax.imshow(img.reshape(30, 60), cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.reshape(-1, 1, 30, 60)\n",
    "horizontal_shift_transform = RandomHorizontalShift(max_shift=60)\n",
    "shifted_dataset = horizontal_shift_transform(test_dataset)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    img = shifted_dataset[i]\n",
    "    ax.set_title(f\"Img {i}\")\n",
    "    ax.imshow(img.reshape(30, 60), cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizontal_flip_transform = RandomHorizontalFlip(consistent=os.truncate, p=0.5)\n",
    "flipped_dataset = horizontal_flip_transform(test_dataset)\n",
    "\n",
    "for i in range(len(test_dataset)):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    origin = test_dataset[i]\n",
    "    flipped = flipped_dataset[i]\n",
    "\n",
    "    axes[0].set_title(\"Original\")\n",
    "    axes[0].imshow(origin.reshape(30, 60), cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "    axes[1].set_title(\"Flipped\")\n",
    "    axes[1].imshow(flipped.reshape(30, 60), cmap=\"gray\", vmin=0, vmax=255)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 254.94path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 254.73path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 255.97path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 256.23path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 255.18path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 261.80path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 251.77path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 256.07path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 254.27path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 239.45path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 260.91path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 250.24path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 263.33path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 258.36path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 257.49path/s]\n",
      "Loading trenton bath house: 100%|██████████| 100/100 [00:00<00:00, 259.49path/s]\n",
      "Loading trenton bath house: 100%|██████████| 100/100 [00:00<00:00, 249.51path/s]\n",
      "Loading trenton bath house: 100%|██████████| 100/100 [00:00<00:00, 258.38path/s]\n"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        RandomHorizontalFlip(consistent=True, p=0.5),\n",
    "        BrightnessJitter(brightness=[0.5, 5], consistent=True, p=0.5),\n",
    "        RandomHorizontalShift(max_shift=60, p=0.5),\n",
    "    ]\n",
    ")\n",
    "test_transform = BldgDataset(transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1890)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_transform[5][\"t_imgs\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import (\n",
    "    AverageMeter,\n",
    "    save_checkpoint,\n",
    "    Logger,\n",
    "    neq_load_customized,\n",
    "    MultiStepLR_Restart_Multiplier,\n",
    "    calc_topk_accuracy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"net\": \"resnet18\",\n",
    "    \"model\": \"memdpc\",\n",
    "    \"dataset\": \"BldgDataset\",\n",
    "    \"seq_len\": 5,  # to 5\n",
    "    \"num_seq\": 6,  # to 6\n",
    "    \"pred_step\": 3,  # to 2\n",
    "    \"ds\": 3,\n",
    "    \"mem_size\": 512,  # 1024\n",
    "    \"batch_size\": 8,  # 4\n",
    "    \"lr\": 1e-3,\n",
    "    \"wd\": 1e-5,\n",
    "    \"resume\": \"\",\n",
    "    \"pretrain\": \"\",\n",
    "    \"epochs\": 500,\n",
    "    \"start_epoch\": 0,\n",
    "    \"gpu\": \"0,1\",\n",
    "    \"print_freq\": 5,\n",
    "    \"reset_lr\": False,\n",
    "    \"prefix\": \"tmp\",\n",
    "    \"img_dim\": [30, 60],  # need to change to [30,60]\n",
    "    \"seed\": 0,\n",
    "    \"workers\": 16,\n",
    "    \"p\": 0.8,  # probability to apply data augmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbone.select_backbone import select_resnet\n",
    "from backbone.convrnn import ConvGRU\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet2d3d_full(\n",
      "  (conv1): Conv3d(1, 64, kernel_size=(1, 5, 5), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
      "  (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock2d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock2d(\n",
      "      (conv1): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock2d(\n",
      "      (conv1): Conv3d(64, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock2d(\n",
      "      (conv1): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock3d(\n",
      "      (conv1): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
      "      (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# test shape\n",
    "from backbone.resnet_2d3d import resnet18_2d3d_full\n",
    "\n",
    "resModel = resnet18_2d3d_full()\n",
    "print(resModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([96, 256, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# x: video input, size = [BatchSize,Channel,TimeSteps,Height,Width],\n",
    "# e.g. [16*6,1,5,30,60]\n",
    "test_tensor = torch.randn(96, 1, 5, 30, 60)\n",
    "output = resModel(test_tensor)\n",
    "# Print the output size\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "print([int(math.ceil(x / 32)) for x in [30, 60]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemDPC_BD(nn.Module):\n",
    "    \"\"\"MemDPC with bi-directional RNN\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_size=[30, 60],\n",
    "        num_seq=8,\n",
    "        seq_len=5,\n",
    "        pred_step=3,\n",
    "        network=\"resnet18\",\n",
    "        mem_size=1024,\n",
    "    ):\n",
    "        super(MemDPC_BD, self).__init__()\n",
    "        print(\n",
    "            \"Using MemDPC-BiDirectional model with {} and mem_size {}\".format(\n",
    "                network, mem_size\n",
    "            )\n",
    "        )\n",
    "        self.sample_size = sample_size\n",
    "        self.num_seq = num_seq\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_step = pred_step\n",
    "        self.last_duration = int(math.ceil(seq_len / 4))\n",
    "        self.last_size = [int(math.ceil(x / 32)) for x in sample_size]\n",
    "        self.mem_size = mem_size\n",
    "        self.tgt_dict = {}\n",
    "        print(\n",
    "            \"final feature map has size %dx%d\" % (self.last_size[0], self.last_size[1])\n",
    "        )\n",
    "\n",
    "        self.backbone, self.param = select_resnet(\n",
    "            network\n",
    "        )  # param come from this function\n",
    "        self.param[\"num_layers\"] = 1  # param for GRU\n",
    "        self.param[\"hidden_size\"] = self.param[\"feature_size\"]  # param for GRU\n",
    "        self.param[\"membanks_size\"] = mem_size\n",
    "        self.mb = torch.nn.Parameter(\n",
    "            torch.randn(self.param[\"membanks_size\"], self.param[\"feature_size\"])\n",
    "        )\n",
    "        print(\n",
    "            \"MEM Bank has size %dx%d\"\n",
    "            % (self.param[\"membanks_size\"], self.param[\"feature_size\"])\n",
    "        )\n",
    "\n",
    "        # bi-directional RNN\n",
    "        self.agg_f = ConvGRU(\n",
    "            input_size=self.param[\"feature_size\"],\n",
    "            hidden_size=self.param[\"hidden_size\"],\n",
    "            kernel_size=1,\n",
    "            num_layers=self.param[\"num_layers\"],\n",
    "        )\n",
    "        self.agg_b = ConvGRU(\n",
    "            input_size=self.param[\"feature_size\"],\n",
    "            hidden_size=self.param[\"hidden_size\"],\n",
    "            kernel_size=1,\n",
    "            num_layers=self.param[\"num_layers\"],\n",
    "        )\n",
    "\n",
    "        self.network_pred = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                self.param[\"feature_size\"],\n",
    "                self.param[\"feature_size\"],\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(\n",
    "                self.param[\"feature_size\"],\n",
    "                self.param[\"membanks_size\"],\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "        )\n",
    "        self.mask = None\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "        self._initialize_weights(self.agg_f)\n",
    "        self._initialize_weights(self.agg_b)\n",
    "        self._initialize_weights(self.network_pred)\n",
    "\n",
    "    def get_loss(self, pred, gt, B, SL, last_size, feature_size, kernel=1):\n",
    "        # pred: B,C,N,H,W\n",
    "        # GT: C,B,N,H*H\n",
    "        score = torch.matmul(\n",
    "            pred.permute(0, 2, 3, 4, 1)\n",
    "            .contiguous()\n",
    "            .view(B * SL * last_size[0] * last_size[1], feature_size),\n",
    "            gt.contiguous().view(feature_size, B * SL * last_size[0] * last_size[1]),\n",
    "        )\n",
    "        if SL not in self.tgt_dict:\n",
    "            self.tgt_dict[SL] = torch.arange(B * SL * last_size[0] * last_size[1])\n",
    "        tgt = self.tgt_dict[SL].to(score.device)\n",
    "        loss = self.ce_loss(score, tgt)\n",
    "        top1, top5 = calc_topk_accuracy(score, tgt, (1, 5))\n",
    "        return loss, top1, top5\n",
    "\n",
    "    def forward(self, block):  # input_seq\n",
    "        # extract feature\n",
    "        (\n",
    "            B,\n",
    "            N,\n",
    "            C,\n",
    "            SL,\n",
    "            H,\n",
    "            W,\n",
    "        ) = (\n",
    "            block.shape\n",
    "        )  # batch size, blocks?, channel, number of steps(consecutive frames used for prediction), H,W\n",
    "        block = block.view(B * N, C, SL, H, W)\n",
    "        feat3d = self.backbone(block)  # put into resNet to extract features\n",
    "\n",
    "        feat3d = F.avg_pool3d(feat3d, (self.last_duration, 1, 1), stride=(1, 1, 1))\n",
    "        feat3d = feat3d.view(\n",
    "            B, N, self.param[\"feature_size\"], self.last_size[0], self.last_size[1]\n",
    "        )  # before ReLU, (-inf, +inf)\n",
    "\n",
    "        losses = []  # all loss\n",
    "        acc = []  # all acc\n",
    "        loss = 0\n",
    "        gt = (\n",
    "            feat3d.permute(2, 0, 1, 3, 4)\n",
    "            .contiguous()\n",
    "            .view(\n",
    "                self.param[\"feature_size\"], B, N, self.last_size[0] * self.last_size[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        feat3d_b = torch.flip(feat3d, dims=(1,))\n",
    "        gt_b = torch.flip(gt, dims=(2,))\n",
    "\n",
    "        # forward MemDPC\n",
    "        pd_tmp_pool = []\n",
    "        for j in range(self.pred_step):\n",
    "            if j == 0:\n",
    "                feat_tmp = feat3d[:, 0 : (N - self.pred_step), :, :, :]\n",
    "                _, hidden = self.agg_f(F.relu(feat_tmp))\n",
    "                context_feature = hidden.clone()\n",
    "            else:\n",
    "                _, hidden = self.agg_f(F.relu(pd_tmp).unsqueeze(1), hidden.unsqueeze(0))\n",
    "            hidden = hidden[\n",
    "                :, -1, :\n",
    "            ]  # after tanh, (-1,1). get the hidden state of last layer, last time step\n",
    "            pd_tmp = self.network_pred(hidden)\n",
    "            pd_tmp = F.softmax(pd_tmp, dim=1)  # B,MEM,H,W\n",
    "            pd_tmp = torch.einsum(\"bmhw,mc->bchw\", pd_tmp, self.mb)\n",
    "            pd_tmp_pool.append(pd_tmp)\n",
    "\n",
    "        pd_tmp_pool = torch.stack(pd_tmp_pool, dim=2)\n",
    "        SL_tmp = pd_tmp_pool.size(2)\n",
    "        gt_tmp = gt[:, :, -self.pred_step : :, :]\n",
    "        loss_tmp, top1, top5 = self.get_loss(\n",
    "            pd_tmp_pool, gt_tmp, B, SL_tmp, self.last_size, self.param[\"feature_size\"]\n",
    "        )\n",
    "        loss_tmp = loss_tmp.mean()\n",
    "        loss = loss_tmp\n",
    "        losses.append(loss_tmp.data.unsqueeze(0))\n",
    "        acc.append(torch.stack([top1, top5], 0).unsqueeze(0))\n",
    "\n",
    "        # backward MemDPC\n",
    "        pd_tmp_pool_b = []\n",
    "        for j in range(self.pred_step):\n",
    "            if j == 0:\n",
    "                feat_tmp = feat3d_b[:, 0 : (N - self.pred_step), :, :, :]\n",
    "                _, hidden = self.agg_b(F.relu(feat_tmp))\n",
    "            else:\n",
    "                _, hidden = self.agg_b(\n",
    "                    F.relu(pd_tmp_b).unsqueeze(1), hidden.unsqueeze(0)\n",
    "                )\n",
    "            hidden = hidden[\n",
    "                :, -1, :\n",
    "            ]  # after tanh, (-1,1). get the hidden state of last layer, last time step\n",
    "            pd_tmp_b = self.network_pred(hidden)\n",
    "            pd_tmp_b = F.softmax(pd_tmp_b, dim=1)  # B,MEM,H,W\n",
    "            pd_tmp_b = torch.einsum(\"bmhw,mc->bchw\", pd_tmp_b, self.mb)\n",
    "            pd_tmp_pool_b.append(pd_tmp_b)\n",
    "\n",
    "        pd_tmp_pool_b = torch.stack(pd_tmp_pool_b, dim=2)\n",
    "        SL_tmp = pd_tmp_pool_b.size(2)\n",
    "        gt_tmp_b = gt_b[:, :, -self.pred_step : :, :]\n",
    "        loss_tmp_b, top1, top5 = self.get_loss(\n",
    "            pd_tmp_pool_b,\n",
    "            gt_tmp_b,\n",
    "            B,\n",
    "            SL_tmp,\n",
    "            self.last_size,\n",
    "            self.param[\"feature_size\"],\n",
    "        )\n",
    "        loss_tmp_b = loss_tmp_b.mean()\n",
    "        losses.append(loss_tmp_b.data.unsqueeze(0))\n",
    "        acc.append(torch.stack([top1, top5], 0).unsqueeze(0))\n",
    "\n",
    "        loss = loss + loss_tmp_b\n",
    "\n",
    "        return loss, losses, acc, context_feature\n",
    "\n",
    "    def extract_features(self, block):\n",
    "        (B, N, C, SL, H, W) = block.shape\n",
    "        block = block.view(B * N, C, SL, H, W)\n",
    "        feat3d = self.backbone(block)\n",
    "        feat3d = F.avg_pool3d(feat3d, (self.last_duration, 1, 1), stride=(1, 1, 1))\n",
    "        feat3d = feat3d.view(\n",
    "            B, N, self.param[\"feature_size\"], self.last_size[0], self.last_size[1]\n",
    "        )\n",
    "\n",
    "        feat_tmp = feat3d[:, 0 : (N - self.pred_step), :, :, :]\n",
    "        _, hidden = self.agg_f(F.relu(feat_tmp))\n",
    "        context_feature = hidden.clone()\n",
    "\n",
    "        return context_feature\n",
    "\n",
    "    def _initialize_weights(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if \"bias\" in name:\n",
    "                nn.init.constant_(param, 0.0)\n",
    "            elif \"weight\" in name:\n",
    "                nn.init.orthogonal_(param, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_data(transform, args=args, mode=\"train\"):\n",
    "    print(\"Loading {} dataset for {}\".format(args[\"dataset\"], mode))\n",
    "\n",
    "    dataset = BldgDataset(\n",
    "        mode=mode,\n",
    "        transform=transform,\n",
    "        seq_len=args[\"seq_len\"],\n",
    "        num_seq=args[\"num_seq\"],\n",
    "    )\n",
    "\n",
    "    shuffle = mode == \"train\"\n",
    "    if shuffle:\n",
    "        sampler = data.RandomSampler(dataset)\n",
    "    else:\n",
    "        sampler = None\n",
    "\n",
    "    data_loader = data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args[\"batch_size\"],\n",
    "        sampler=sampler,\n",
    "        shuffle=shuffle\n",
    "        and sampler\n",
    "        is None,  # Only shuffle if training and no other sampler is specified\n",
    "        num_workers=args[\"workers\"],\n",
    "        pin_memory=True,\n",
    "        drop_last=True,  # Typically for ensuring consistent batch sizes, especially during training\n",
    "    )\n",
    "\n",
    "    print('\"{}\" dataset size: {}'.format(mode, len(dataset)))\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_path(args):\n",
    "    if args[\"resume\"]:\n",
    "        exp_path = os.path.dirname(os.path.dirname(args[\"resume\"]))\n",
    "    else:\n",
    "        exp_path = (\n",
    "            f\"log_{args['prefix']}/{args['model']}_{args['dataset']}-{args['img_dim'][0]}_{args['img_dim'][1]}_{args['net']}_\"\n",
    "            f\"mem{args['mem_size']}_bs{args['batch_size']}_lr{args['lr']}_seq{args['num_seq']}_pred{args['pred_step']}_\"\n",
    "            f\"len{args['seq_len']}_ds{args['ds']}\"\n",
    "        )\n",
    "\n",
    "    img_path = os.path.join(exp_path, \"img\")\n",
    "    model_path = os.path.join(exp_path, \"model\")\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    return img_path, model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    data_loader, model, criterion, optimizer, lr_scheduler, device, epoch, args\n",
    "):\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    accuracy = [\n",
    "        [AverageMeter(), AverageMeter()],  # forward top1, top5\n",
    "        [AverageMeter(), AverageMeter()],\n",
    "    ]  # backward top1, top5\n",
    "\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "    tic = time.time()\n",
    "\n",
    "    for idx, input_seq in enumerate(data_loader):\n",
    "        # print('inside the data_loader loop now')\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        input_seq = input_seq.to(device)\n",
    "\n",
    "        B = input_seq[\"t_imgs\"].size(0)\n",
    "        loss, loss_step, acc, extra = model(input_seq[\"t_imgs\"])\n",
    "\n",
    "        for i in range(2):\n",
    "            top1, top5 = acc[i].mean(0)  # average acc across multi-gpus\n",
    "            accuracy[i][0].update(top1.item(), B)\n",
    "            accuracy[i][1].update(top5.item(), B)\n",
    "\n",
    "        loss = loss.mean()  # average loss across multi-gpus\n",
    "        losses.update(loss.item(), B)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if idx % args[\"print_freq\"] == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Loss {loss.val:.6f}\\t\"\n",
    "                \"Acc: {acc[0][0].val:.4f}\\t\"\n",
    "                \"T-data:{dt.val:.2f} T-batch:{bt.val:.2f}\\t\".format(\n",
    "                    epoch,\n",
    "                    idx,\n",
    "                    len(data_loader),\n",
    "                    loss=losses,\n",
    "                    acc=accuracy,\n",
    "                    dt=data_time,\n",
    "                    bt=batch_time,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            args[\"writer_train\"].add_scalar(\"local/loss\", losses.val, args[\"iteration\"])\n",
    "            args[\"writer_train\"].add_scalar(\n",
    "                \"local/F-top1\", accuracy[0][0].val, args[\"iteration\"]\n",
    "            )\n",
    "            args[\"writer_train\"].add_scalar(\n",
    "                \"local/F-top5\", accuracy[0][1].val, args[\"iteration\"]\n",
    "            )\n",
    "            args[\"writer_train\"].add_scalar(\n",
    "                \"local/B-top1\", accuracy[1][0].val, args[\"iteration\"]\n",
    "            )\n",
    "            args[\"writer_train\"].add_scalar(\n",
    "                \"local/B-top5\", accuracy[1][1].val, args[\"iteration\"]\n",
    "            )\n",
    "\n",
    "        args[\"iteration\"] += 1\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "    print(\"Epoch: [{0}]\\t\" \"T-epoch:{t:.2f}\\t\".format(epoch, t=time.time() - tic))\n",
    "\n",
    "    args[\"writer_train\"].add_scalar(\"global/loss\", losses.avg, epoch)\n",
    "    args[\"writer_train\"].add_scalar(\"global/F-top1\", accuracy[0][0].avg, epoch)\n",
    "    args[\"writer_train\"].add_scalar(\"global/F-top5\", accuracy[0][1].avg, epoch)\n",
    "    args[\"writer_train\"].add_scalar(\"global/B-top1\", accuracy[1][0].avg, epoch)\n",
    "    args[\"writer_train\"].add_scalar(\"global/B-top5\", accuracy[1][1].avg, epoch)\n",
    "\n",
    "    return losses.avg, np.mean([accuracy[0][0].avg, accuracy[1][0].avg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data_loader, model, criterion, device, epoch, args):\n",
    "    losses = AverageMeter()\n",
    "    accuracy = [\n",
    "        [AverageMeter(), AverageMeter()],  # forward top1, top5\n",
    "        [AverageMeter(), AverageMeter()],\n",
    "    ]  # backward top1, top5\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, input_seq in enumerate(data_loader):\n",
    "            input_seq = input_seq.to(device)\n",
    "            B = input_seq[\"t_imgs\"].size(0)\n",
    "            loss, loss_step, acc, extra = model(input_seq[\"t_imgs\"])\n",
    "\n",
    "            for i in range(2):\n",
    "                top1, top5 = acc[i].mean(0)  # average acc across multi-gpus\n",
    "                accuracy[i][0].update(top1.item(), B)\n",
    "                accuracy[i][1].update(top5.item(), B)\n",
    "\n",
    "            loss = loss.mean()  # average loss across multi-gpus\n",
    "            losses.update(loss.item(), B)\n",
    "\n",
    "    print(\n",
    "        \"Epoch: [{0}/{1}]\\t\"\n",
    "        \"Loss {loss.val:.6f}\\t\"\n",
    "        \"Acc: {acc[0][0].val:.4f}\\t\".format(\n",
    "            epoch, args[\"epochs\"], loss=losses, acc=accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "    args[\"writer_val\"].add_scalar(\"global/loss\", losses.avg, epoch)\n",
    "    args[\"writer_val\"].add_scalar(\"global/F-top1\", accuracy[0][0].avg, epoch)\n",
    "    args[\"writer_val\"].add_scalar(\"global/F-top5\", accuracy[0][1].avg, epoch)\n",
    "    args[\"writer_val\"].add_scalar(\"global/B-top1\", accuracy[1][0].avg, epoch)\n",
    "    args[\"writer_val\"].add_scalar(\"global/B-top5\", accuracy[1][1].avg, epoch)\n",
    "\n",
    "    return losses.avg, np.mean([accuracy[0][0].avg, accuracy[1][0].avg])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    random.seed(args[\"seed\"])\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args[\"gpu\"])\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_gpu = len(str(args[\"gpu\"]).split(\",\"))\n",
    "    args[\"batch_size\"] = num_gpu * args[\"batch_size\"]\n",
    "\n",
    "    ### model ###\n",
    "    # need to test:\n",
    "    # num_seq to 10?  seq_len to 3\n",
    "    if args[\"model\"] == \"memdpc\":\n",
    "        model = MemDPC_BD(\n",
    "            sample_size=args[\"img_dim\"],\n",
    "            num_seq=args[\"num_seq\"],\n",
    "            seq_len=args[\"seq_len\"],\n",
    "            network=args[\"net\"],\n",
    "            pred_step=args[\"pred_step\"],\n",
    "            mem_size=args[\"mem_size\"],\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\"wrong model!\")\n",
    "\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    model_without_dp = model.module\n",
    "\n",
    "    ### optimizer ###\n",
    "    params = model.parameters()\n",
    "    optimizer = optim.Adam(params, lr=args[\"lr\"], weight_decay=args[\"wd\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### data ###\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            RandomHorizontalFlip(consistent=True, p=args[\"p\"]),\n",
    "            BrightnessJitter(brightness=[0.5, 5], consistent=True, p=args[\"p\"]),\n",
    "            RandomHorizontalShift(max_shift=60, p=args[\"p\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = None\n",
    "\n",
    "    train_loader = get_data(train_transform, args=args, mode=\"train\")\n",
    "    val_loader = get_data(val_transform, args=args, mode=\"val\")\n",
    "\n",
    "    lr_milestones_eps = [50, 100]  # can be smaller\n",
    "\n",
    "    lr_milestones = [len(train_loader) * m for m in lr_milestones_eps]\n",
    "    print(\n",
    "        \"=> Use lr_scheduler: %s eps == %s iters\"\n",
    "        % (str(lr_milestones_eps), str(lr_milestones))\n",
    "    )\n",
    "    lr_lambda = lambda ep: MultiStepLR_Restart_Multiplier(\n",
    "        ep, gamma=0.1, step=lr_milestones, repeat=1\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    best_acc = 0\n",
    "    args[\"iteration\"] = 1\n",
    "\n",
    "    ### restart training ###\n",
    "    if args[\"resume\"]:\n",
    "        if os.path.isfile(args[\"resume\"]):\n",
    "            print(\"=> loading resumed checkpoint '{}'\".format(args[\"resume\"]))\n",
    "            checkpoint = torch.load(args[\"resume\"], map_location=torch.device(\"cpu\"))\n",
    "            args[\"start_epoch\"] = checkpoint[\"epoch\"]\n",
    "            args[\"iteration\"] = checkpoint[\"iteration\"]\n",
    "            best_acc = checkpoint[\"best_acc\"]\n",
    "            model_without_dp.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            except:\n",
    "                print(\"[WARNING] Not loading optimizer states\")\n",
    "            print(\n",
    "                \"=> loaded resumed checkpoint '{}' (epoch {})\".format(\n",
    "                    args[\"resume\"], checkpoint[\"epoch\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\"[Warning] no checkpoint found at '{}'\".format(args[\"resume\"]))\n",
    "            sys.exit(0)\n",
    "\n",
    "    # logging tools\n",
    "    args[\"img_path\"], args[\"model_path\"] = set_path(args)\n",
    "    args[\"logger\"] = Logger(path=args[\"img_path\"])\n",
    "    args[\"logger\"].log(\n",
    "        \"args=\\n\\t\\t\"\n",
    "        + \"\\n\\t\\t\".join([\"%s:%s\" % (str(k), str(v)) for k, v in args.items()])\n",
    "    )\n",
    "\n",
    "    args[\"writer_val\"] = SummaryWriter(logdir=os.path.join(args[\"img_path\"], \"val\"))\n",
    "    args[\"writer_train\"] = SummaryWriter(logdir=os.path.join(args[\"img_path\"], \"train\"))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    ### main loop ###\n",
    "    for epoch in range(args[\"start_epoch\"], args[\"epochs\"]):\n",
    "        np.random.seed(epoch)\n",
    "        random.seed(epoch)\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            train_loader, model, criterion, optimizer, lr_scheduler, device, epoch, args\n",
    "        )\n",
    "        val_loss, val_acc = validate(val_loader, model, criterion, device, epoch, args)\n",
    "\n",
    "        # save check_point\n",
    "        is_best = val_acc > best_acc\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": model_without_dp.state_dict(),\n",
    "            \"best_acc\": best_acc,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"iteration\": args[\"iteration\"],\n",
    "        }\n",
    "        save_checkpoint(\n",
    "            save_dict,\n",
    "            is_best,\n",
    "            filename=os.path.join(args[\"model_path\"], \"epoch%s.pth.tar\" % str(epoch)),\n",
    "            keep_all=False,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"Training from ep %d to ep %d finished\" % (args[\"start_epoch\"], args[\"epochs\"])\n",
    "    )\n",
    "    # sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_args(args, mem_size=512, epochs=250, batch_size=4, p=0.5, lr=5e-3, wd=1e-4):\n",
    "    args[\"mem_size\"] = mem_size\n",
    "    args[\"epochs\"] = epochs\n",
    "    args[\"workers\"] = 12\n",
    "    args[\"batch_size\"] = batch_size\n",
    "    args[\"p\"] = p\n",
    "    args[\"lr\"] = lr\n",
    "    args[\"wd\"] = wd\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MemDPC-BiDirectional model with resnet18 and mem_size 512\n",
      "final feature map has size 1x2\n",
      "MEM Bank has size 512x256\n",
      "Loading BldgDataset dataset for train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 261.91path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 250.94path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 253.05path/s]\n",
      "Loading caracalla baths: 100%|██████████| 100/100 [00:00<00:00, 258.64path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 256.00path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 259.32path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 254.05path/s]\n",
      "Loading india institute of management: 100%|██████████| 100/100 [00:00<00:00, 238.41path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 256.05path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 246.91path/s]\n",
      "Loading pantheon: 100%|██████████| 100/100 [00:00<00:00, 266.18path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 252.14path/s]\n",
      "Loading trajans market: 100%|██████████| 100/100 [00:00<00:00, 239.12path/s]\n"
     ]
    }
   ],
   "source": [
    "change_args(args)\n",
    "main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
