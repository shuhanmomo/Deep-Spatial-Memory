{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unit function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading demo: 100%|██████████| 300/300 [00:01<00:00, 245.10path/s]\n",
      "Loading demo: 100%|██████████| 300/300 [00:01<00:00, 247.04path/s]\n"
     ]
    }
   ],
   "source": [
    "from helpers.dataset import BldgDataset\n",
    "\n",
    "dataset = BldgDataset(\"./DemoExperiment/1117/demo_test.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import random\n",
    "\n",
    "\n",
    "class RandomCropWithProb:\n",
    "    def __init__(self, size, p=0.8, consistent=True):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = (size[0], size[1])\n",
    "        self.consistent = consistent\n",
    "        self.threshold = p\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        img1 = imgmap[0]\n",
    "        h, w = img1.shape\n",
    "        print(w)\n",
    "        if self.size is not None:\n",
    "            th, tw = self.size\n",
    "            print(tw)\n",
    "            print(w - tw)\n",
    "            if w == tw and h == th:\n",
    "                return imgmap\n",
    "            if self.consistent:\n",
    "                if random.random() < self.threshold:\n",
    "                    x1 = random.randint(0, w - tw)\n",
    "                    y1 = random.randint(0, h - th)\n",
    "                else:\n",
    "                    x1 = int(round((w - tw) / 2.0))\n",
    "                    y1 = int(round((h - th) / 2.0))\n",
    "                return [i[y1 : y1 + th, x1 : x1 + tw] for i in imgmap]\n",
    "            else:\n",
    "                result = []\n",
    "                for i in imgmap:\n",
    "                    if random.random() < self.threshold:\n",
    "                        x1 = random.randint(0, w - tw)\n",
    "                        y1 = random.randint(0, h - th)\n",
    "                    else:\n",
    "                        x1 = int(round((w - tw) / 2.0))\n",
    "                        y1 = int(round((h - th) / 2.0))\n",
    "                    result.append(i[y1 : y1 + th, x1 : x1 + tw])\n",
    "                return result\n",
    "        else:\n",
    "            return imgmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "\n",
    "class Scale:\n",
    "    def __init__(self, size, interpolation=cv2.INTER_NEAREST):\n",
    "        assert isinstance(size, int) or (\n",
    "            isinstance(size, collections.Iterable) and len(size) == 2\n",
    "        )\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        resized_images = []\n",
    "        for img in imgmap:\n",
    "            h, w = img.shape[:2]\n",
    "            if isinstance(self.size, int):\n",
    "                if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                    resized_images.append(img)\n",
    "                    continue\n",
    "                if w < h:\n",
    "                    ow = self.size\n",
    "                    oh = int(self.size * h / w)\n",
    "                else:\n",
    "                    oh = self.size\n",
    "                    ow = int(self.size * w / h)\n",
    "                resized_img = cv2.resize(\n",
    "                    img, (ow, oh), interpolation=self.interpolation\n",
    "                )\n",
    "            else:\n",
    "                resized_img = cv2.resize(\n",
    "                    img, (self.size[1], self.size[0]), interpolation=self.interpolation\n",
    "                )\n",
    "            resized_images.append(resized_img)\n",
    "        return resized_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading demo: 100%|██████████| 300/300 [00:01<00:00, 250.13path/s]\n",
      "Loading demo: 100%|██████████| 300/300 [00:01<00:00, 261.44path/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = BldgDataset(data_path=\"./DemoExperiment/1117/demo_test.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 30, 60)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\moumo\\OneDrive\\Deep Spatial Memory\\test_network.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m cropped_transform \u001b[39m=\u001b[39m RandomCropWithProb(size\u001b[39m=\u001b[39m[\u001b[39m25\u001b[39m, \u001b[39m50\u001b[39m], p\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, consistent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m scale_transform \u001b[39m=\u001b[39m Scale(size\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m, \u001b[39m60\u001b[39m))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cropped_data \u001b[39m=\u001b[39m cropped_transform(test_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m resized_data \u001b[39m=\u001b[39m scale_transform(cropped_data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(resized_data)):\n",
      "File \u001b[1;32mc:\\Users\\moumo\\OneDrive\\Deep Spatial Memory\\helpers\\augmentation.py:166\u001b[0m, in \u001b[0;36mRandomCropWithProb.__call__\u001b[1;34m(self, imgmap)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, imgmap):\n\u001b[0;32m    165\u001b[0m     img1 \u001b[39m=\u001b[39m imgmap[\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 166\u001b[0m     _, h, w \u001b[39m=\u001b[39m img1\u001b[39m.\u001b[39mshape\n\u001b[0;32m    167\u001b[0m     \u001b[39mprint\u001b[39m(img1\u001b[39m.\u001b[39mshape)\n\u001b[0;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "test_data = dataset[0][\"imgs\"][:10]\n",
    "print(test_data.shape)\n",
    "\n",
    "cropped_transform = RandomCropWithProb(size=[25, 50], p=1.0, consistent=True)\n",
    "scale_transform = Scale(size=(30, 60))\n",
    "cropped_data = cropped_transform(test_data)\n",
    "resized_data = scale_transform(cropped_data)\n",
    "for i in range(len(resized_data)):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    img = cropped_data[i]\n",
    "    ax.set_title(f\"img {i}\")\n",
    "    ax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing folder path00\n",
      "Finished processing folder path01\n",
      "Finished processing folder path02\n",
      "Finished processing folder path03\n",
      "Finished processing folder path04\n",
      "Finished processing folder path05\n",
      "Finished processing folder path06\n",
      "Finished processing folder path07\n",
      "Finished processing folder path08\n",
      "Finished processing folder path09\n",
      "Finished processing folder path10\n",
      "Finished processing folder path11\n",
      "Finished processing folder path12\n",
      "Finished processing folder path13\n",
      "Finished processing folder path14\n",
      "Finished processing folder path15\n",
      "Finished processing folder path16\n",
      "Finished processing folder path17\n",
      "Finished processing folder path18\n",
      "Finished processing folder path19\n",
      "Finished processing folder path20\n",
      "Finished processing folder path21\n",
      "Finished processing folder path22\n",
      "Finished processing folder path23\n",
      "Finished processing folder path24\n",
      "Finished processing folder path25\n",
      "Finished processing folder path26\n",
      "Finished processing folder path27\n",
      "Finished processing folder path28\n",
      "Finished processing folder path29\n",
      "Finished processing folder path30\n",
      "Finished processing folder path31\n",
      "Finished processing folder path32\n",
      "Finished processing folder path33\n",
      "Finished processing folder path34\n",
      "Finished processing folder path35\n",
      "Finished processing folder path36\n",
      "Finished processing folder path37\n",
      "Finished processing folder path38\n",
      "Finished processing folder path39\n",
      "Finished processing folder path40\n",
      "Finished processing folder path41\n",
      "Finished processing folder path42\n",
      "Finished processing folder path43\n",
      "Finished processing folder path44\n",
      "Finished processing folder path45\n",
      "Finished processing folder path46\n",
      "Finished processing folder path47\n",
      "Finished processing folder path48\n",
      "Finished processing folder path49\n",
      "Finished processing folder path50\n",
      "Finished processing folder path51\n",
      "Finished processing folder path52\n",
      "Finished processing folder path53\n",
      "Finished processing folder path54\n",
      "Finished processing folder path55\n",
      "Finished processing folder path56\n",
      "Finished processing folder path57\n",
      "Finished processing folder path58\n",
      "Finished processing folder path59\n",
      "Finished processing folder path60\n",
      "Finished processing folder path61\n",
      "Finished processing folder path62\n",
      "Finished processing folder path63\n",
      "Finished processing folder path64\n",
      "Finished processing folder path65\n",
      "Finished processing folder path66\n",
      "Finished processing folder path67\n",
      "Finished processing folder path68\n",
      "Finished processing folder path69\n",
      "Finished processing folder path70\n",
      "Finished processing folder path71\n",
      "Finished processing folder path72\n",
      "Finished processing folder path73\n",
      "Finished processing folder path74\n",
      "Finished processing folder path75\n",
      "Finished processing folder path76\n",
      "Finished processing folder path77\n",
      "Finished processing folder path78\n",
      "Finished processing folder path79\n",
      "Finished processing folder path80\n",
      "Finished processing folder path81\n",
      "Finished processing folder path82\n",
      "Finished processing folder path83\n",
      "Finished processing folder path84\n",
      "Finished processing folder path85\n",
      "Finished processing folder path86\n",
      "Finished processing folder path87\n",
      "Finished processing folder path88\n",
      "Finished processing folder path89\n",
      "Finished processing folder path90\n",
      "Finished processing folder path91\n",
      "Finished processing folder path92\n",
      "Finished processing folder path93\n",
      "Finished processing folder path94\n",
      "Finished processing folder path95\n",
      "Finished processing folder path96\n",
      "Finished processing folder path97\n",
      "Finished processing folder path98\n",
      "Finished processing folder path99\n",
      "Finished processing folder path100\n",
      "Finished processing folder path101\n",
      "Finished processing folder path102\n",
      "Finished processing folder path103\n",
      "Finished processing folder path104\n",
      "Finished processing folder path105\n",
      "Finished processing folder path106\n",
      "Finished processing folder path107\n",
      "Finished processing folder path108\n",
      "Finished processing folder path109\n",
      "Finished processing folder path110\n",
      "Finished processing folder path111\n",
      "Finished processing folder path112\n",
      "Finished processing folder path113\n",
      "Finished processing folder path114\n",
      "Finished processing folder path115\n",
      "Finished processing folder path116\n",
      "Finished processing folder path117\n",
      "Finished processing folder path118\n",
      "Finished processing folder path119\n",
      "Finished processing folder path120\n",
      "Finished processing folder path121\n",
      "Finished processing folder path122\n",
      "Finished processing folder path123\n",
      "Finished processing folder path124\n",
      "Finished processing folder path125\n",
      "Finished processing folder path126\n",
      "Finished processing folder path127\n",
      "Finished processing folder path128\n",
      "Finished processing folder path129\n",
      "Finished processing folder path130\n",
      "Finished processing folder path131\n",
      "Finished processing folder path132\n",
      "Finished processing folder path133\n",
      "Finished processing folder path134\n",
      "Finished processing folder path135\n",
      "Finished processing folder path136\n",
      "Finished processing folder path137\n",
      "Finished processing folder path138\n",
      "Finished processing folder path139\n",
      "Finished processing folder path140\n",
      "Finished processing folder path141\n",
      "Finished processing folder path142\n",
      "Finished processing folder path143\n",
      "Finished processing folder path144\n",
      "Finished processing folder path145\n",
      "Finished processing folder path146\n",
      "Finished processing folder path147\n",
      "Finished processing folder path148\n",
      "Finished processing folder path149\n",
      "Finished processing folder path150\n",
      "Finished processing folder path151\n",
      "Finished processing folder path152\n",
      "Finished processing folder path153\n",
      "Finished processing folder path154\n",
      "Finished processing folder path155\n",
      "Finished processing folder path156\n",
      "Finished processing folder path157\n",
      "Finished processing folder path158\n",
      "Finished processing folder path159\n",
      "Finished processing folder path160\n",
      "Finished processing folder path161\n",
      "Finished processing folder path162\n",
      "Finished processing folder path163\n",
      "Finished processing folder path164\n",
      "Finished processing folder path165\n",
      "Finished processing folder path166\n",
      "Finished processing folder path167\n",
      "Finished processing folder path168\n",
      "Finished processing folder path169\n",
      "Finished processing folder path170\n",
      "Finished processing folder path171\n",
      "Finished processing folder path172\n",
      "Finished processing folder path173\n",
      "Finished processing folder path174\n",
      "Finished processing folder path175\n",
      "Finished processing folder path176\n",
      "Finished processing folder path177\n",
      "Finished processing folder path178\n",
      "Finished processing folder path179\n",
      "Finished processing folder path180\n",
      "Finished processing folder path181\n",
      "Finished processing folder path182\n",
      "Finished processing folder path183\n",
      "Finished processing folder path184\n",
      "Finished processing folder path185\n",
      "Finished processing folder path186\n",
      "Finished processing folder path187\n",
      "Finished processing folder path188\n",
      "Finished processing folder path189\n",
      "Finished processing folder path190\n",
      "Finished processing folder path191\n",
      "Finished processing folder path192\n",
      "Finished processing folder path193\n",
      "Finished processing folder path194\n",
      "Finished processing folder path195\n",
      "Finished processing folder path196\n",
      "Finished processing folder path197\n",
      "Finished processing folder path198\n",
      "Finished processing folder path199\n",
      "Finished processing folder path200\n",
      "Finished processing folder path201\n",
      "Finished processing folder path202\n",
      "Finished processing folder path203\n",
      "Finished processing folder path204\n",
      "Finished processing folder path205\n",
      "Finished processing folder path206\n",
      "Finished processing folder path207\n",
      "Finished processing folder path208\n",
      "Finished processing folder path209\n",
      "Finished processing folder path210\n",
      "Finished processing folder path211\n",
      "Finished processing folder path212\n",
      "Finished processing folder path213\n",
      "Finished processing folder path214\n",
      "Finished processing folder path215\n",
      "Finished processing folder path216\n",
      "Finished processing folder path217\n",
      "Finished processing folder path218\n",
      "Finished processing folder path219\n",
      "Finished processing folder path220\n",
      "Finished processing folder path221\n",
      "Finished processing folder path222\n",
      "Finished processing folder path223\n",
      "Finished processing folder path224\n",
      "Finished processing folder path225\n",
      "Finished processing folder path226\n",
      "Finished processing folder path227\n",
      "Finished processing folder path228\n",
      "Finished processing folder path229\n",
      "Finished processing folder path230\n",
      "Finished processing folder path231\n",
      "Finished processing folder path232\n",
      "Finished processing folder path233\n",
      "Finished processing folder path234\n",
      "Finished processing folder path235\n",
      "Finished processing folder path236\n",
      "Finished processing folder path237\n",
      "Finished processing folder path238\n",
      "Finished processing folder path239\n",
      "Finished processing folder path240\n",
      "Finished processing folder path241\n",
      "Finished processing folder path242\n",
      "Finished processing folder path243\n",
      "Finished processing folder path244\n",
      "Finished processing folder path245\n",
      "Finished processing folder path246\n",
      "Finished processing folder path247\n",
      "Finished processing folder path248\n",
      "Finished processing folder path249\n",
      "Finished processing folder path250\n",
      "Finished processing folder path251\n",
      "Finished processing folder path252\n",
      "Finished processing folder path253\n",
      "Finished processing folder path254\n",
      "Finished processing folder path255\n",
      "Finished processing folder path256\n",
      "Finished processing folder path257\n",
      "Finished processing folder path258\n",
      "Finished processing folder path259\n",
      "Finished processing folder path260\n",
      "Finished processing folder path261\n",
      "Finished processing folder path262\n",
      "Finished processing folder path263\n",
      "Finished processing folder path264\n",
      "Finished processing folder path265\n",
      "Finished processing folder path266\n",
      "Finished processing folder path267\n",
      "Finished processing folder path268\n",
      "Finished processing folder path269\n",
      "Finished processing folder path270\n",
      "Finished processing folder path271\n",
      "Finished processing folder path272\n",
      "Finished processing folder path273\n",
      "Finished processing folder path274\n",
      "Finished processing folder path275\n",
      "Finished processing folder path276\n",
      "Finished processing folder path277\n",
      "Finished processing folder path278\n",
      "Finished processing folder path279\n",
      "Finished processing folder path280\n",
      "Finished processing folder path281\n",
      "Finished processing folder path282\n",
      "Finished processing folder path283\n",
      "Finished processing folder path284\n",
      "Finished processing folder path285\n",
      "Finished processing folder path286\n",
      "Finished processing folder path287\n",
      "Finished processing folder path288\n",
      "Finished processing folder path289\n",
      "Finished processing folder path290\n",
      "Finished processing folder path291\n",
      "Finished processing folder path292\n",
      "Finished processing folder path293\n",
      "Finished processing folder path294\n",
      "Finished processing folder path295\n",
      "Finished processing folder path296\n",
      "Finished processing folder path297\n",
      "Finished processing folder path298\n",
      "Finished processing folder path299\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "\n",
    "def preprocess_image_data(base_path, zip_name):\n",
    "    # Unzip the file\n",
    "    zip_path = os.path.join(base_path, zip_name)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(base_path)\n",
    "\n",
    "    # Iterate through each 'pathXX' folder and rename PNGs using a temporary name\n",
    "    for i in range(300):\n",
    "        folder_name = f\"path{str(i).zfill(2)}\"\n",
    "        folder_path = os.path.join(base_path, folder_name)\n",
    "\n",
    "        # First, rename to temporary names\n",
    "        for j in range(30):\n",
    "            old_name = f\"panoramic_{str(j).zfill(2)}.png\"\n",
    "            temp_name = f\"temp_{str(j).zfill(2)}.png\"\n",
    "            old_path = os.path.join(folder_path, old_name)\n",
    "            temp_path = os.path.join(folder_path, temp_name)\n",
    "\n",
    "            if os.path.exists(old_path):\n",
    "                os.rename(old_path, temp_path)\n",
    "\n",
    "        # Then, rename temporary files to the final names\n",
    "        for j in range(30):\n",
    "            temp_name = f\"temp_{str(j).zfill(2)}.png\"\n",
    "            new_name = f\"panoramic_{str(29-j).zfill(2)}.png\"\n",
    "            temp_path = os.path.join(folder_path, temp_name)\n",
    "            new_path = os.path.join(folder_path, new_name)\n",
    "\n",
    "            if os.path.exists(temp_path):\n",
    "                os.rename(temp_path, new_path)\n",
    "\n",
    "        print(f\"Finished processing folder {folder_name}\")\n",
    "\n",
    "    # Rezip the folders with the new name\n",
    "    new_zip_name_parts = zip_name[:-4].split(\"_\")  # Split the original zip name\n",
    "    new_zip_name_parts[1], new_zip_name_parts[2] = (\n",
    "        new_zip_name_parts[2],\n",
    "        new_zip_name_parts[1],\n",
    "    )  # Swap 'start' and 'end'\n",
    "    new_zip_name = \"_\".join(new_zip_name_parts) + \".zip\"\n",
    "\n",
    "    with zipfile.ZipFile(os.path.join(base_path, new_zip_name), \"w\") as zipf:\n",
    "        for i in range(300):\n",
    "            folder_name = f\"path{str(i).zfill(2)}\"\n",
    "            folder_path = os.path.join(base_path, folder_name)\n",
    "            for root, dirs, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    zipf.write(\n",
    "                        os.path.join(root, file),\n",
    "                        os.path.relpath(os.path.join(root, file), base_path),\n",
    "                    )\n",
    "\n",
    "    # Optional: Delete the path folders after re-zipping\n",
    "    for i in range(300):\n",
    "        shutil.rmtree(os.path.join(base_path, f\"path{str(i).zfill(2)}\"))\n",
    "\n",
    "\n",
    "# Usage\n",
    "preprocess_image_data(\"./DemoExperiment/process\", \"demo_exterior_room_a.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import zipfile\n",
    "import sklearn\n",
    "import shutil\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "plt.switch_backend(\"agg\")\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, models, transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from helpers.augmentation import (\n",
    "    BrightnessJitter,\n",
    "    RandomHorizontalFlip,\n",
    "    Scale,\n",
    "    RandomCropWithProb,\n",
    ")\n",
    "from helpers.dataset import BldgDataset\n",
    "from helpers.utils import (\n",
    "    AverageMeter,\n",
    "    save_checkpoint,\n",
    "    Logger,\n",
    "    neq_load_customized,\n",
    "    MultiStepLR_Restart_Multiplier,\n",
    "    calc_topk_accuracy,\n",
    ")\n",
    "from backbone.select_backbone import select_resnet\n",
    "from backbone.memdpc import MemDPC_BD\n",
    "from helpers.training_utils import get_data, set_path, train_one_epoch, validate\n",
    "\n",
    "args = json.load(open(\"./config/demo_config.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    torch.manual_seed(args[\"seed\"])\n",
    "    np.random.seed(args[\"seed\"])\n",
    "    random.seed(args[\"seed\"])\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args[\"gpu\"])\n",
    "    device = torch.device(\"cuda\")\n",
    "    num_gpu = len(str(args[\"gpu\"]).split(\",\"))\n",
    "    print(f\"num_gpu:{num_gpu}\")\n",
    "    # args[\"batch_size\"] = num_gpu * args[\"batch_size\"]\n",
    "\n",
    "    ### model ###\n",
    "    # need to test:\n",
    "    # num_seq to 10?  seq_len to 3\n",
    "    if args[\"model\"] == \"memdpc\":\n",
    "        model = MemDPC_BD(\n",
    "            sample_size=args[\"img_dim\"],\n",
    "            num_seq=args[\"num_seq\"],\n",
    "            seq_len=args[\"seq_len\"],\n",
    "            network=args[\"net\"],\n",
    "            pred_step=args[\"pred_step\"],\n",
    "            mem_size=args[\"mem_size\"],\n",
    "            drop_out=args[\"drop_out\"],\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\"wrong model!\")\n",
    "\n",
    "    model.to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    model_without_dp = model.module\n",
    "\n",
    "    ### optimizer ###\n",
    "    params = model.parameters()\n",
    "    optimizer = optim.Adam(params, lr=args[\"lr\"], weight_decay=args[\"wd\"])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### data ###\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            RandomCropWithProb(size=[25, 50], p=args[\"p\"], consistent=True),\n",
    "            Scale(size=(30, 60)),\n",
    "            RandomHorizontalFlip(consistent=True, p=args[\"p\"]),\n",
    "            BrightnessJitter(brightness=[0.5, 3], consistent=True, p=args[\"p\"]),\n",
    "            # RandomHorizontalShift(max_shift=60, p=args[\"p\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    val_transform = None\n",
    "\n",
    "    train_loader = get_data(train_transform, args=args, mode=\"train\")\n",
    "    val_loader = get_data(val_transform, args=args, mode=\"val\")\n",
    "\n",
    "    lr_milestones_eps = [50, 100]  # can be smaller\n",
    "\n",
    "    lr_milestones = [len(train_loader) * m for m in lr_milestones_eps]\n",
    "    print(\n",
    "        \"=> Use lr_scheduler: %s eps == %s iters\"\n",
    "        % (str(lr_milestones_eps), str(lr_milestones))\n",
    "    )\n",
    "    lr_lambda = lambda ep: MultiStepLR_Restart_Multiplier(\n",
    "        ep, gamma=0.1, step=lr_milestones, repeat=1\n",
    "    )\n",
    "    lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "    best_acc = 0\n",
    "    args[\"iteration\"] = 1\n",
    "\n",
    "    ### restart training ###\n",
    "    if args[\"resume\"]:\n",
    "        if os.path.isfile(args[\"resume\"]):\n",
    "            print(\"=> loading resumed checkpoint '{}'\".format(args[\"resume\"]))\n",
    "            checkpoint = torch.load(args[\"resume\"], map_location=torch.device(\"cpu\"))\n",
    "            args[\"start_epoch\"] = checkpoint[\"epoch\"]\n",
    "            args[\"iteration\"] = checkpoint[\"iteration\"]\n",
    "            best_acc = checkpoint[\"best_acc\"]\n",
    "            model_without_dp.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            try:\n",
    "                optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            except:\n",
    "                print(\"[WARNING] Not loading optimizer states\")\n",
    "            print(\n",
    "                \"=> loaded resumed checkpoint '{}' (epoch {})\".format(\n",
    "                    args[\"resume\"], checkpoint[\"epoch\"]\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            print(\"[Warning] no checkpoint found at '{}'\".format(args[\"resume\"]))\n",
    "            sys.exit(0)\n",
    "\n",
    "    # logging tools\n",
    "    args[\"img_path\"], args[\"model_path\"] = set_path(args)\n",
    "    args[\"logger\"] = Logger(path=args[\"img_path\"])\n",
    "    args[\"logger\"].log(\n",
    "        \"args=\\n\\t\\t\"\n",
    "        + \"\\n\\t\\t\".join([\"%s:%s\" % (str(k), str(v)) for k, v in args.items()])\n",
    "    )\n",
    "\n",
    "    args[\"writer_val\"] = SummaryWriter(logdir=os.path.join(args[\"img_path\"], \"val\"))\n",
    "    args[\"writer_train\"] = SummaryWriter(logdir=os.path.join(args[\"img_path\"], \"train\"))\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    ### main loop ###\n",
    "    for epoch in range(args[\"start_epoch\"], args[\"epochs\"]):\n",
    "        np.random.seed(epoch)\n",
    "        random.seed(epoch)\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            train_loader, model, criterion, optimizer, lr_scheduler, device, epoch, args\n",
    "        )\n",
    "        val_loss, val_acc = validate(val_loader, model, criterion, device, epoch, args)\n",
    "\n",
    "        # save check_point\n",
    "        is_best = val_acc > best_acc\n",
    "        best_acc = max(val_acc, best_acc)\n",
    "        save_dict = {\n",
    "            \"epoch\": epoch,\n",
    "            \"state_dict\": model_without_dp.state_dict(),\n",
    "            \"best_acc\": best_acc,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"iteration\": args[\"iteration\"],\n",
    "        }\n",
    "        save_checkpoint(\n",
    "            save_dict,\n",
    "            is_best,\n",
    "            filename=os.path.join(args[\"model_path\"], \"epoch%s.pth.tar\" % str(epoch)),\n",
    "            keep_all=False,\n",
    "        )\n",
    "        args[\"best_val_acc\"] = best_acc\n",
    "\n",
    "    print(\n",
    "        \"Training from ep %d to ep %d finished\" % (args[\"start_epoch\"], args[\"epochs\"])\n",
    "    )\n",
    "    # sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_args(\n",
    "    args,\n",
    "    mem_size=128,\n",
    "    epochs=60,\n",
    "    batch_size=8,\n",
    "    p=0.65,\n",
    "    lr=5e-4,\n",
    "    wd=1e-4,\n",
    "    drop_out=0.3,\n",
    "    data_path=\"./DemoExperiment/1117/demo_test.zip\",\n",
    "):  # try batch size\n",
    "    args[\"mem_size\"] = mem_size\n",
    "    args[\"epochs\"] = epochs\n",
    "    # args['workers'] = 12\n",
    "    args[\"batch_size\"] = batch_size\n",
    "    args[\"p\"] = p\n",
    "    args[\"lr\"] = lr\n",
    "    args[\"wd\"] = wd\n",
    "    args[\"data_path\"] = data_path\n",
    "    args[\"workers\"] = 2\n",
    "    args[\"drop_out\"] = drop_out\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_gpu:2\n",
      "Using MemDPC-BiDirectional model with resnet18_simplified and mem_size 128\n",
      "final feature map has size 1x2\n",
      "MEM Bank has size 128x128\n",
      "Loading BldgDataset dataset for train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading demo: 100%|██████████| 300/300 [00:01<00:00, 243.35path/s]\n",
      "Loading demo: 100%|██████████| 300/300 [00:01<00:00, 225.74path/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"train\" dataset size: 420\n",
      "Loading BldgDataset dataset for val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading demo: 100%|██████████| 300/300 [00:03<00:00, 85.79path/s]\n",
      "Loading demo: 100%|██████████| 300/300 [00:03<00:00, 92.10path/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"val\" dataset size: 90\n",
      "=> Use lr_scheduler: [50, 100] eps == [2600, 5200] iters\n",
      "Epoch: [0][0/52]\tLoss 11.698868\tAcc: 0.0208\tT-data:5.95 T-batch:9.04\t\n",
      "Epoch: [0][5/52]\tLoss 19.051331\tAcc: 0.0208\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][10/52]\tLoss 9.588809\tAcc: 0.0417\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][15/52]\tLoss 9.796906\tAcc: 0.0417\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][20/52]\tLoss 7.234867\tAcc: 0.0208\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][25/52]\tLoss 6.851819\tAcc: 0.0417\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][30/52]\tLoss 6.170689\tAcc: 0.0417\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][35/52]\tLoss 5.975263\tAcc: 0.0833\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][40/52]\tLoss 5.628435\tAcc: 0.1042\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][45/52]\tLoss 5.605404\tAcc: 0.0833\tT-data:0.00 T-batch:0.06\t\n",
      "Epoch: [0][50/52]\tLoss 5.277074\tAcc: 0.1667\tT-data:0.00 T-batch:0.07\t\n",
      "Epoch: [0]\tT-epoch:12.71\t\n",
      "Validation:\n",
      "Epoch: [0/60]\tLoss 5.291866\tAcc: 0.0833\t\n",
      "Epoch: [1][0/52]\tLoss 5.194293\tAcc: 0.1042\tT-data:4.14 T-batch:4.16\t\n",
      "Epoch: [1][5/52]\tLoss 5.198958\tAcc: 0.0833\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][10/52]\tLoss 4.991960\tAcc: 0.0625\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][15/52]\tLoss 5.038644\tAcc: 0.1250\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][20/52]\tLoss 5.124966\tAcc: 0.0833\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][25/52]\tLoss 5.018581\tAcc: 0.1042\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][30/52]\tLoss 5.087945\tAcc: 0.1042\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][35/52]\tLoss 5.852487\tAcc: 0.0625\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][40/52]\tLoss 5.532894\tAcc: 0.0625\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1][45/52]\tLoss 5.298828\tAcc: 0.0417\tT-data:0.00 T-batch:0.02\t\n",
      "Epoch: [1][50/52]\tLoss 5.164198\tAcc: 0.1250\tT-data:0.00 T-batch:0.03\t\n",
      "Epoch: [1]\tT-epoch:6.08\t\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "args = change_args(args)\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.dataset import BldgDataset\n",
    "from helpers.augmentation import RandomHorizontalFlip, BrightnessJitter\n",
    "import cv2\n",
    "from torchvision import datasets, models, transforms\n",
    "import numbers\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCropWithProb:\n",
    "    def __init__(self, size, p=0.8, consistent=True):\n",
    "        if isinstance(size, numbers.Number):\n",
    "            self.size = (int(size), int(size))\n",
    "        else:\n",
    "            self.size = (size[0], size[1])\n",
    "        self.consistent = consistent\n",
    "        self.threshold = p\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        cropped_images = []\n",
    "        for img in imgmap:\n",
    "            h, w = img.shape[-2:]  # Assuming img shape is (C, H, W)\n",
    "            th, tw = self.size\n",
    "\n",
    "            if w == tw and h == th:\n",
    "                cropped_images.append(img)\n",
    "                continue\n",
    "\n",
    "            if self.consistent:\n",
    "                if random.random() < self.threshold:\n",
    "                    x1 = random.randint(0, w - tw)\n",
    "                    y1 = random.randint(0, h - th)\n",
    "                else:\n",
    "                    x1 = int(round((w - tw) / 2.0))\n",
    "                    y1 = int(round((h - th) / 2.0))\n",
    "                cropped_img = img[:, y1 : y1 + th, x1 : x1 + tw]\n",
    "            else:\n",
    "                if random.random() < self.threshold:\n",
    "                    x1 = random.randint(0, w - tw)\n",
    "                    y1 = random.randint(0, h - th)\n",
    "                else:\n",
    "                    x1 = int(round((w - tw) / 2.0))\n",
    "                    y1 = int(round((h - th) / 2.0))\n",
    "                cropped_img = img[:, y1 : y1 + th, x1 : x1 + tw]\n",
    "\n",
    "            cropped_images.append(cropped_img)\n",
    "\n",
    "        return np.stack(cropped_images, axis=0)\n",
    "\n",
    "\n",
    "class Scale:\n",
    "    def __init__(self, size, interpolation=cv2.INTER_NEAREST):\n",
    "        assert isinstance(size, int) or (len(size) == 2)\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def __call__(self, imgmap):\n",
    "        resized_images = []\n",
    "        for img in imgmap:\n",
    "            h, w = img.shape[-2:]  # Assuming img shape is (C, H, W)\n",
    "            if isinstance(self.size, int):\n",
    "                if (w <= h and w == self.size) or (h <= w and h == self.size):\n",
    "                    resized_images.append(img)\n",
    "                    continue\n",
    "                if w < h:\n",
    "                    ow = self.size\n",
    "                    oh = int(self.size * h / w)\n",
    "                else:\n",
    "                    oh = self.size\n",
    "                    ow = int(self.size * w / h)\n",
    "                resized_img = cv2.resize(\n",
    "                    img[0], (ow, oh), interpolation=self.interpolation\n",
    "                )\n",
    "                resized_img = np.expand_dims(\n",
    "                    resized_img, axis=0\n",
    "                )  # Add channel dimension back\n",
    "            else:\n",
    "                resized_img = cv2.resize(\n",
    "                    img[0],\n",
    "                    (self.size[1], self.size[0]),\n",
    "                    interpolation=self.interpolation,\n",
    "                )\n",
    "                resized_img = np.expand_dims(\n",
    "                    resized_img, axis=0\n",
    "                )  # Add channel dimension back\n",
    "\n",
    "            resized_images.append(resized_img)\n",
    "\n",
    "        return np.stack(resized_images, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "args = json.load(open(\"./config/demo_config.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        RandomCropWithProb(size=[25, 50], p=args[\"p\"], consistent=True),\n",
    "        Scale(size=(30, 60)),\n",
    "        RandomHorizontalFlip(consistent=True, p=args[\"p\"]),\n",
    "        BrightnessJitter(brightness=[0.5, 3], consistent=True, p=args[\"p\"]),\n",
    "        # RandomHorizontalShift(max_shift=60, p=args[\"p\"]),\n",
    "    ]\n",
    ")\n",
    "test_data = BldgDataset(\n",
    "    data_path=\"./DemoExperiment/1117/demo_test.zip\", transform=train_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\moumo\\OneDrive\\Deep Spatial Memory\\test_network.ipynb Cell 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m  \u001b[39m# Example batch size\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m batch \u001b[39m=\u001b[39m create_batch(test_data, batch_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m transformed_batch \u001b[39m=\u001b[39m apply_transformations_to_batch(batch, train_transform)\n",
      "\u001b[1;32mc:\\Users\\moumo\\OneDrive\\Deep Spatial Memory\\test_network.ipynb Cell 18\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_transformations_to_batch\u001b[39m(batch, transform):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Assuming the transform can handle a batch of images\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     transformed_batch \u001b[39m=\u001b[39m transform(batch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m transformed_batch\n",
      "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dsm\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;32mc:\\Users\\moumo\\OneDrive\\Deep Spatial Memory\\test_network.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         cropped_img \u001b[39m=\u001b[39m img[:, y1 : y1 \u001b[39m+\u001b[39m th, x1 : x1 \u001b[39m+\u001b[39m tw]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     cropped_images\u001b[39m.\u001b[39mappend(cropped_img)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/moumo/OneDrive/Deep%20Spatial%20Memory/test_network.ipynb#X33sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mstack(cropped_images, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\moumo\\anaconda3\\envs\\dsm\\lib\\site-packages\\numpy\\core\\shape_base.py:449\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out, dtype, casting)\u001b[0m\n\u001b[0;32m    447\u001b[0m shapes \u001b[39m=\u001b[39m {arr\u001b[39m.\u001b[39mshape \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays}\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shapes) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mall input arrays must have the same shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    451\u001b[0m result_ndim \u001b[39m=\u001b[39m arrays[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mndim \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    452\u001b[0m axis \u001b[39m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[1;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
